Thank you for your thorough review.  We believe all your concerns have
been addressed below.

> One general note is that a good amount of time is spent re-hashing
> previous results and previously derived methods. Pages 2-7 fit this
> description. Some use of referencing is done, but I believe this
> doesn't go far enough, and that much is written down that does not
> need to be.
>
> There is some pedalogical use in this in that it saves the reader
> the trouble of searching out some references: however, I still thing
> this could be made briefer.

This section has been compacted a fair amount (now occupies a bit less
than revised pages 2-5).  We would be happy to add additional
references to the methods used in VPIC and/or to other codes using
these or related methods though.  However, we are loathe to do any
futher compaction of this section; this paper is intended to serve
three purposes (in order of importance).

- The modeling capabilities and performance results shown ideally
should be sufficient for people outside the PIC simulation community
to do a back-of-the-envelope calculation to determine if next
generation codes and implementation techniques could help them solve
intractable problems in their plasma physics specialty.  As some of
the readers of PoP are not PIC experts, they would benefit from, at
least, a minimal discussion of the motivations and limitations for the
methods used in PIC in general and VPIC specifically.

- The paper should serve as an authoritative reference for modeling
capabilities and limitations of VPIC.  Removing too much here will do
more than inconvenience readers.  Lacking an authoritative VPIC
reference will make it difficult for the plasma physics community to
assess the fidelity and/or reproduce results in the growing body of
published work using VPIC.

- Reproducibility of the specific performance results presented in the
paper.  As you note in your later comments, the benefits of
implementation techniques described are somewhat sensitive to what is
being simulated (algorithms and physics parameters).  Omitting these
details can mislead readers versed in PIC simulation about which of
these techniques apply to them.

> The paper seems to be aimed at making the point that the VPIC
> algorithm is highly efficient within certain restrictions, however:
>
> (1) HOW efficient isn't really quantified in a useful way--yes, a
> certain piece of hardware achieves X performance, but what
> percentage of theoretical performance is achieved? Theoretical
> performance should probably be measured NOT in FLOPs but rather % of
> theoretical memory bandwidth.

Done.  In section III.G:

``~49% of measured peak memory bandwidth and ~21% of theoretical
aggregate SPE single precision floating point performance''

> (2) HOW efficient the code performs when it is not operating on its
> ideal data set: how much performance degradation occurs if the
> plasma is drifting with respect to the mesh? What is the cost of
> exceptional particle processing compared to normal processing?

Drifting plasma and related issues were addressed in response (4)
below.  Exceptional processing has been addressed in Section III.F:

``Exception processing itself does not use any dramatic techniques.
In 3d, exception processing time typically scales as the simulation
domain surface area (and any boundaries within that domain); this is
negligible compared to the particle advance time which scales as the
simulation domain volume.''

> Both (1) and (2) are key for a reader to understand the limits of
> the technique and the likely benefits of applying a similar
> technique to his problem.
> 
> Further:
>
> (3) In the section titled "Field interpolation and particle
> sorting", first paragraph, last sentence, "...much larger
> granularity under the hood..." ... *I* understand what is meant
> here, but the general reader may not be aware of the fact that the
> underlying hardware may grab an entire cache line when memory is
> requested rather than just what was wanted.

Done.  We did not want to get into more detailed discussion of how
memory and caches work originally.  Based on your request, this has
been replaced with (in section III.D):

``Further exacerbating this, far more data than requested would be
transfered; modern cores round-up the amount of memory transfered in
small requests like this to, at least, the (larger) memory-core dath
path width.''

Hopefully, this will make the idea clear to a general reader without
requiring a digression into the complex details of how memory requests
are actually fulfilled by modern cores.

> (4) In the introduction, there is a sentence, "...techniques used in
> VPIC .... can be used in many other simulation codes." As the paper
> is written, this claim seems perhaps overly broad. Strictly
> speaking, yes, the techniques can be used anywhere, but the domains
> where they actually are optimal are more limited. Specifically, how
> about very high temperature and very high drift velocities? Highly
> nonuniform plasmas? A linear accelerator, for example, has a very
> cold beam, which is very localized in space: VPICs implementation
> isn't designed for this situation, though it is actually possible
> that VPIC's implementation will outperform many existing codes even
> here.

Done.  This sentence has been replaced by (in section I):

``The techniques used in VPIC, outlined below, can be applied to other
PIC simulations in widespread use in plasma physics and have even been
used with success in radically different fields such as computational
biochemisty (see, for example, Ref. [3]).''

For reference, Ref. [3] is the authoritative reference to the
computional biochemistry code architected by one of the authors using
similar techniques to those outlined in this paper that was previously
cited in this paper.

You are correct that VPIC still can outperform other codes even when
running in suboptimal regimes.  The following explanations were added
to the section.

In section III.A:

``For example, in particle dominated simulations, VPIC has smoothly
scaled to some of the largest machines available (several thousand
nodes) without any special tuning effort when the simulation domain
could be statically decomposed to yield an approximately uniform
number of particles per node (even some highly non-uniform plasmas can
be so decomposed, especially if using variable particle charges).''

In section III.D:

``This field interpolation approach is less effective when there are
very few particles per cell on average or when a large fraction of the
cells are devoid of particles.  In these cases, the field
interpolation coefficient computation time is not compensated by a
reduction in the particle field interpolation time.  However, given
that the cost of forming unused coefficients is small and that cells
containing multiple particles still see some benefit, this effect is
mild.  Further, this regime is atypical; sampling of $f_s$ well
typically requires many particles per cell on average.

``High temperature particle distributions generally require more
frequent sorting to maximize overall performance than cold particle
distributions.  Fortunately, the finite speed of light in relativistic
simulation (or, more generally, timestep limitations due to numerical
aliasing instabilities induced by particles moving through too many
cells in a step) limit the rate at which the particle array becomes
disorganized at even the highest temperatures.  Interestingly,
non-thermal particle distributions (for example, fast moving cold
drifting particle beams) tend to need less performance sorting; as
these stay more organized physically, they stay more organized in
memory as well.  Also, the frequent sorting that occurs when using
various particle-particle collision models usually eliminates the need
for additional performance sorting.''

In section III.F:

``Other particle distributions may have different performance
characteristics, but, as noted above, simulation stability criteria
(especially in 3d) generally limit the number of cell boundaries a
particle will cross in a timestep to a few with most not crossing at
all.  Further, in many simulations with a drifting particle species,
the simulation frame can be chosen to reduce such potential
performance impacts---this can even improve numerical quality and
yield significantly less stringent simulation stability criteria
[18].''

For reference, [18] is:

@Article{Vay_2007,
  author =  {J.~L.~Vay},
  title =   {Noninvariance of Space- and Time-Scale Ranges under a Lorentz Transformation and the Implications for the Study of Relativistic Interactions},
  journal = {Phys.~Rev.~Lett.},
  year =    {2007},
  volume =  {98},
  number =  {130405},
  month =   {30 March}
}

> (5) If possible, more details of the plasma physics of the three
> applications could be discussed, since they would be of interest to
> the readers of PoP.

We agree they are of interest to PoP readers and we would love to talk
more about them (some articles cited in this section include authors
of this paper and were published by PoP).  However, given this paper
was to cover the associated APS-DPP invited talk on PIC simulation
methods, we felt it inappropriate to focus on previously published
applications of VPIC over current unpublished algorithmic and
implementation details of VPIC.  Even if we were to focus exclusively
on previous applications, given the limited space, it would not be
possible to give a sufficient amount of detail to satisfy an
application area expert for any individual application.  Being limited
to a brief overview, our intent is to demonstrate more concretely the
kinds of problems that next generation simulation capabilities will be
able to tackle.  Hopefully, readers desiring more details about the
specific applications will find the cited papers satisfy this need.
